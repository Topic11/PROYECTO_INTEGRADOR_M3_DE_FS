# ğŸš€ Pipeline de Datos ELT para YouTube Trending en GCP

## 1. Resumen del Proyecto

Este proyecto implementa un pipeline de datos End-to-End (E2E) de tipo **ELT (Extract - Load - Transform)** para la ingesta y anÃ¡lisis de datos de videos en tendencia de YouTube. El objetivo es transformar datos crudos de una API en un Data Warehouse estructurado y validado, listo para ser consumido por analistas y responder preguntas de negocio, sentando las bases para una soluciÃ³n de datos robusta y escalable.

---

## 2. Arquitectura y JustificaciÃ³n de Herramientas ğŸ› ï¸

La arquitectura fue diseÃ±ada siguiendo un enfoque **serverless** y de **bajo mantenimiento**, eligiendo cada herramienta para cumplir un rol especÃ­fico y resolver los desafÃ­os de escalabilidad y automatizaciÃ³n planteados en las consignas.

| Componente | Herramienta Elegida | Â¿Por quÃ© se eligiÃ³ esta herramienta? |
| :--- | :--- | :--- |
| **Plataforma Cloud** | **Google Cloud Platform (GCP)** | Se seleccionÃ³ por su ecosistema de servicios de datos serverless, maduros y perfectamente integrados entre sÃ­. |
| **ContenerizaciÃ³n** | **Docker** | **Para garantizar la portabilidad y consistencia.** Empaqueta el pipeline en una imagen autocontenida que funciona igual en cualquier entorno. |
| **EjecuciÃ³n del Pipeline**| **Cloud Run Jobs** | **Para una computaciÃ³n serverless y costo-eficiente.** Ejecuta el contenedor solo bajo demanda, ideal para procesos batch. |
| **Data Lake** | **Google Cloud Storage (GCS)** | **Como almacenamiento de objetos desacoplado, duradero y econÃ³mico.** Sirve como la zona de aterrizaje (`landing zone`) para los datos crudos. |
| **Data Warehouse** | **Google BigQuery** | **Por su escalabilidad masiva y arquitectura serverless.** Es la base perfecta para un DWH que necesita analizar grandes volÃºmenes de datos. |
| **TransformaciÃ³n** | **dbt (Data Build Tool)** | **Para aplicar las mejores prÃ¡cticas de ingenierÃ­a de software a SQL.** Permite construir, testear y documentar los modelos de datos de forma modular y versionable. |
| **OrquestaciÃ³n** | **Cloud Composer (Airflow)**| **Para automatizar y monitorear el pipeline.** Permite definir el pipeline como cÃ³digo (DAGs), programar su ejecuciÃ³n y gestionar reintentos. |
| **CI/CD** | **GitHub y GitHub Actions**| **Para garantizar la calidad y mantenibilidad del cÃ³digo.** Automatiza la ejecuciÃ³n de tests en cada cambio, asegurando la integridad del proyecto. |

---

## 3. Estructura del Proyecto ğŸ“‚

```
.
â”œâ”€â”€ .dbt/
â”‚   â””â”€â”€ profiles.yml
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pi3_youtube_pipeline.py
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ PI_M3_Analysis.docx
â”œâ”€â”€ youtube_dbt_project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ gold/
â”‚   â”‚   â”œâ”€â”€ sources.yml
â”‚   â”‚   â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ seeds/
â”‚   â”‚   â””â”€â”€ dim_region.csv
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ entrypoint.sh
â”œâ”€â”€ requirements.txt
â””â”€â”€ youtubepipeline_e2e.py
```

---

## 4. Modelado de Datos y Tests ğŸ“Š

El Data Warehouse estÃ¡ estructurado en tres capas:

* **Capa `raw` (Bronce):** Datos crudos, sin transformar.
* **Capa `silver` (Plata):** Datos limpios, casteados y estandarizados.
* **Capa `gold` (Oro):** Modelos de datos finales, listos para el consumo (dimensiones y hechos).

**NOTA**: hay un error en la toma de imagen la cual genera los schemas con un prefijo y se muestran como raw_gold y raw_silver cuando solo debiera ser gold y silver.

La **calidad de los datos** se garantiza mediante tests (`not_null`, `unique`, `relationships`) definidos en los archivos `schema.yml` de `dbt`.

---

## 5. AnÃ¡lisis de Datos ğŸ“ˆ

Una vez procesados los datos y almacenados en la capa `gold`, se ejecutaron una serie de consultas SQL para responder a las preguntas de negocio clave del proyecto.

**El detalle completo de las consultas, sus resultados y las conclusiones se encuentra en el siguiente documento:**
### â¡ï¸ **[`docs/PI_M3_Analysis.docx`](docs/PI_M3_Analysis.docx)**

---

## 6. CI/CD con GitHub Actions ğŸ”„

Para garantizar la calidad y mantenibilidad del cÃ³digo, se implementÃ³ un flujo de CI/CD con GitHub Actions. El workflow, definido en `.github/workflows/ci.yml`, se dispara en cada `push` o `pull request` a la rama `main` y ejecuta los siguientes pasos:
1.  Clona el cÃ³digo del repositorio.
2.  Instala `dbt` y sus dependencias (como `dbt_utils`).
3.  Configura las credenciales de BigQuery de forma segura usando un secreto de GitHub.
4.  Ejecuta `dbt build`, que corre los seeds, los modelos y los tests, validando la integridad de todo el proyecto de transformaciÃ³n.

---

## 7. Despliegue y EjecuciÃ³n

El despliegue y la ejecuciÃ³n se realizan mediante comandos de `gcloud` para construir la imagen de Docker, crear el Cloud Run Job y subir el DAG a Composer.

* **EjecuciÃ³n Manual:**
    ```sh
    gcloud run jobs execute pi3-e2e --region=$REGION
    ```
* **EjecuciÃ³n AutomÃ¡tica:**
    El DAG `youtube_daily_pipeline` en Airflow se encarga de la ejecuciÃ³n programada.

---

## 8. Autor

**Federico Strologo**
